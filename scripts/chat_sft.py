"""
Supervised fine-tuning (SFT) the model.
Run as:

python -m scripts.chat_sft

Or torchrun for training:

torchrun --standalone --nproc_per_node=8 -m scripts.chat_sft -- --device-batch-size=16
"""

import argparse
import os

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import time
from contextlib import nullcontext
from dataclasses import asdict

import torch
import torch.distributed as dist
import wandb

from nanochat.checkpoint_manager import load_model, save_checkpoint
from nanochat.common import (
    DummyWandb,
    autodetect_device_type,
    compute_cleanup,
    compute_gradient_stats,
    compute_init,
    get_base_dir,
    get_num_recur_for_microstep,
    print0,
    sample_num_recurs_for_step,
)
from nanochat.dataloader import sft_data_loader
from nanochat.loss_eval import evaluate_bpb
from nanochat.tokenizer import get_token_bytes
from tasks.common import TaskMixture
from tasks.customjson import CustomJSON
from tasks.gsm8k import GSM8K
from tasks.mmlu import MMLU
from tasks.smoltalk import SmolTalk
from tasks.spellingbee import SimpleSpelling, SpellingBee

# -----------------------------------------------------------------------------
# CLI arguments
parser = argparse.ArgumentParser(description="Supervised fine-tuning (SFT) the model")
# Logging
parser.add_argument("--run", type=str, default="dummy", help="wandb run name ('dummy' disables wandb logging)")
# Runtime
parser.add_argument("--device-type", type=str, default="", help="cuda|cpu|mps (empty = autodetect)")
parser.add_argument("--dtype", type=str, default="bfloat16", help="float32|bfloat16")
# Model loading
parser.add_argument("--model-tag", type=str, default=None, help="model tag to load from")
parser.add_argument("--model-step", type=int, default=None, help="model step to load from")
parser.add_argument("--output-tag", type=str, default=None, help="model tag to save to (defaults to model-tag)")
# Training horizon
parser.add_argument("--num-iterations", type=int, default=-1, help="number of optimization steps (-1 = full epoch)")
# Batch sizes
parser.add_argument("--max-seq-len", type=int, default=2048, help="max context length")
parser.add_argument("--device-batch-size", type=int, default=32, help="per-device batch size")
parser.add_argument("--total-batch-size", type=int, default=524288, help="total batch size in tokens")
# Optimization
parser.add_argument("--embedding-lr", type=float, default=0.2, help="learning rate for embedding parameters (Adam)")
parser.add_argument("--unembedding-lr", type=float, default=0.004, help="learning rate for unembedding parameters (Adam)")
parser.add_argument("--matrix-lr", type=float, default=0.02, help="learning rate for matrix parameters (Muon)")
parser.add_argument("--weight-decay", type=float, default=0.0, help="weight decay for embedding/unembedding parameters (Adam)")
parser.add_argument("--init-lr-frac", type=float, default=1.0, help="initial LR as fraction of base LR")
# Evaluation
parser.add_argument("--eval-every", type=int, default=150, help="evaluate val bpb every N steps (-1 = disable)")
parser.add_argument("--eval-tokens", type=int, default=20 * 524288, help="number of tokens to evaluate val loss on")
# Recurrence
parser.add_argument(
    "--recur-samples-per-step",
    type=int,
    default=8,
    help="Number of different num_recur values per gradient accumulation step (None = fixed, use model default)",
)
parser.add_argument(
    "--recur-mean",
    type=float,
    default=None,
    help="Override train_recur_mean for recurrence sampling (default: use model config value)",
)
parser.add_argument(
    "--recur-max",
    type=int,
    default=None,
    help="Override train_recur_max for recurrence sampling (default: use model config value)",
)
# Exit gate (Ouro-style learned depth allocation) â€” overrides checkpoint config when set
parser.add_argument("--exit-beta", type=float, default=None, help="entropy regularization weight for exit gate (None = keep checkpoint value)")
parser.add_argument("--exit-min-recur", type=int, default=None, help="minimum recurrences before exit gate can stop (None = keep checkpoint value)")
parser.add_argument("--exit-log-stats", action=argparse.BooleanOptionalAction, default=False, help="log exit gate stats periodically")
# Output
parser.add_argument("--dry-run", action="store_true", help="log to wandb but skip checkpoints/report")
# Gradient tracking
parser.add_argument(
    "--track-gradients",
    type=str,
    choices=["none", "basic", "detailed"],
    default="basic",
    help="Gradient tracking level: none (disabled), basic (global norm), detailed (per-component norms)",
)
args = parser.parse_args()
user_config = vars(args).copy()
# -----------------------------------------------------------------------------

# Compute init
device_type = autodetect_device_type() if args.device_type == "" else args.device_type
ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)
master_process = ddp_rank == 0
ptdtype = torch.float32 if args.dtype == "float32" else torch.bfloat16
autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype) if device_type == "cuda" else nullcontext()
synchronize = torch.cuda.synchronize if device_type == "cuda" else lambda: None
get_max_memory = torch.cuda.max_memory_allocated if device_type == "cuda" else lambda: 0

# wandb logging init
use_dummy_wandb = args.run == "dummy" or not master_process
wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project="nanochat-sft", name=args.run, config=user_config)

# Load the model and tokenizer (always from base)
model, tokenizer, meta = load_model("base", device, phase="train", model_tag=args.model_tag, step=args.model_step)
pretrain_batch_size = meta.get("device_batch_size", None)
if pretrain_batch_size is not None and args.device_batch_size > pretrain_batch_size:
    print0(
        f"FOOTGUN WARNING: base model training used device_batch_size {pretrain_batch_size}, did you pass in a good --device-batch-size to this script?"
    )
orig_model = model
if args.recur_mean is not None:
    print0(f"Overriding train_recur_mean: {model.config.train_recur_mean} -> {args.recur_mean}")
    model.config.train_recur_mean = args.recur_mean
if args.recur_max is not None:
    print0(f"Overriding train_recur_max: {model.config.train_recur_max} -> {args.recur_max}")
    model.config.train_recur_max = args.recur_max
# Override exit gate config from CLI args (None = keep checkpoint value)
for attr in ("exit_beta", "exit_min_recur"):
    cli_val = getattr(args, attr)
    if cli_val is not None:
        old_val = getattr(model.config, attr)
        setattr(model.config, attr, cli_val)
        print0(f"Overriding model config {attr}: {old_val} -> {cli_val}")
# Use dynamic=False when recur sampling is enabled (varying num_recur causes recompilation otherwise)
model = torch.compile(model, dynamic=bool(args.recur_samples_per_step))
size = model.config.size
num_flops_per_token = model.estimate_flops()
tokens_per_fwdbwd = args.device_batch_size * args.max_seq_len  # tokens per iteration for a single rank
world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size  # total tokens per iteration for all ranks
assert args.total_batch_size % world_tokens_per_fwdbwd == 0
grad_accum_steps = args.total_batch_size // world_tokens_per_fwdbwd
print0(f"Tokens / micro-batch / rank: {args.device_batch_size} x {args.max_seq_len} = {tokens_per_fwdbwd:,}")
print0(f"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}")
print0(f"Total batch size {args.total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}")
# Validate recur-samples-per-step (global across all ranks)
if args.recur_samples_per_step:
    total_micro_steps = ddp_world_size * grad_accum_steps
    if args.recur_samples_per_step > total_micro_steps:
        raise ValueError(
            f"--recur-samples-per-step ({args.recur_samples_per_step}) cannot exceed total micro-steps ({total_micro_steps} = {ddp_world_size} ranks * {grad_accum_steps} steps). "
            f"Decrease --recur-samples-per-step or --device-batch-size."
        )
    if total_micro_steps % args.recur_samples_per_step != 0:
        raise ValueError(
            f"Total micro-steps ({total_micro_steps} = {ddp_world_size} ranks * {grad_accum_steps} steps) must be evenly divisible by --recur-samples-per-step ({args.recur_samples_per_step}). "
            f"Adjust --device-batch-size to change grad_accum_steps."
        )
    microsteps_per_sample = total_micro_steps // args.recur_samples_per_step
    print0(f"Recurrence sampling: {args.recur_samples_per_step} global samples per step ({microsteps_per_sample} microsteps each across {ddp_world_size} ranks)")
else:
    print0(f"Recurrence sampling: fixed (using model default)")
token_bytes = get_token_bytes(device=device)

# Initialize the Optimizer
optimizer = model.setup_optimizer(
    unembedding_lr=args.unembedding_lr,
    embedding_lr=args.embedding_lr,
    matrix_lr=args.matrix_lr,
    weight_decay=args.weight_decay,
)
# Set the initial learning rate as a fraction of the base learning rate
for group in optimizer.param_groups:
    group["lr"] = group["lr"] * args.init_lr_frac
    group["initial_lr"] = group["lr"]

# SFT data mixture and DataLoader
base_dir = get_base_dir()
identity_conversations_filepath = os.path.join(base_dir, "identity_conversations.jsonl")
train_dataset = TaskMixture(
    [
        SmolTalk(split="train"),  # 460K rows of general conversations
        MMLU(subset="auxiliary_train", split="train"),  # 100K rows of multiple choice problems drawn from ARC, MC_TEST, OBQA, RACE
        GSM8K(subset="main", split="train"),  # 8K rows teaching simple math and (calculator) tool use
        GSM8K(subset="main", split="train"),  # 2 epochs of GSM8K
        CustomJSON(filepath=identity_conversations_filepath),  # 1000 rows of synthetic identity conversations
        CustomJSON(filepath=identity_conversations_filepath),  # let's do 2 epochs of these
        SimpleSpelling(size=200000, split="train"),  # 200K rows of Simple Spelling (e.g. spell the word 'apple')
        SpellingBee(size=80000, split="train"),  # 80K rows of Spelling Bee (e.g. how many 'r' are in 'strawberry'?)
    ]
)  # total: 460K + 100K + 16K + 200K + 80K = 856K rows
val_dataset = TaskMixture(
    [
        SmolTalk(split="test"),  # 24K rows in test set
        MMLU(subset="all", split="test", stop=5200),  # 14K rows in test set, use only 5.2K to match the train ratios
        GSM8K(subset="main", split="test", stop=420),  # 1.32K rows in test set, use only 420 to match the train ratios
    ]
)  # total: 24K + 14K + 1.32K ~= 39K rows
# DataLoader is defined here, it emits inputs, targets : 2D tensors of shape (device_batch_size, max_seq_len)
# A big problem is that we don't know the final num_iterations in advance. So we create
# these two global variables and update them from within the data generator.
last_step = False  # we will toggle this to True when we reach the end of the training dataset
approx_progress = 0.0  # will go from 0 to 1 over the course of the epoch
current_epoch = 1  # track epoch for logging


def _tracked_sft_loader(split):
    """Wrap sft_data_loader with progress/epoch tracking for the training loop."""
    global last_step, approx_progress, current_epoch
    assert split in {"train", "val"}, "split must be 'train' or 'val'"
    dataset = train_dataset if split == "train" else val_dataset
    loader = sft_data_loader(
        tokenizer, dataset,
        B=args.device_batch_size, T=args.max_seq_len,
        device=device, device_type=device_type,
        ddp_rank=ddp_rank, ddp_world_size=ddp_world_size,
    )
    it = 0
    for inputs, targets, meta in loader:
        if split == "train":
            it += 1
            consumed, dataset_size = meta["consumed"], meta["dataset_size"]
            current_epoch = consumed // dataset_size + 1
            if args.num_iterations > 0:
                approx_progress = it / args.num_iterations
                if it >= args.num_iterations:
                    last_step = True
            else:
                approx_progress = consumed / dataset_size
                if consumed >= dataset_size:
                    last_step = True
        yield inputs, targets


train_loader = _tracked_sft_loader("train")
build_val_loader = lambda: _tracked_sft_loader("val")
progress = 0  # will go from 0 to 1 over the course of the epoch


# Learning rate scheduler (warmdown: 80% constant, then linear decay)
def get_lr_multiplier(progress):
    # first 80% of training: no decay, then linearly ramp down to 0.
    return 1 if progress < 0.8 else 1 - (progress - 0.8) / 0.2


# Momentum scheduler for Muon optimizer
def get_muon_momentum(it):
    frac = min(it / 300, 1)
    momentum = (1 - frac) * 0.85 + frac * 0.95
    return momentum


# -----------------------------------------------------------------------------
# Training loop
x, y = next(train_loader)  # prefetch the very first batch of data
min_val_bpb = float("inf")
smooth_train_loss = 0  # EMA of training loss
ema_beta = 0.9  # EMA decay factor
total_training_time = 0  # total wall-clock time of training
step = 0
while True:
    flops_so_far = num_flops_per_token * args.total_batch_size * step

    # Synchronize last_step across all ranks to avoid hangs in the distributed setting
    if ddp:
        last_step_tensor = torch.tensor(last_step, dtype=torch.int32, device=device)
        dist.all_reduce(last_step_tensor, op=dist.ReduceOp.MAX)
        last_step = bool(last_step_tensor.item())

    # once in a while: evaluate the val bpb (all ranks participate)
    if last_step or (args.eval_every > 0 and step % args.eval_every == 0):
        model.eval()
        val_loader = build_val_loader()
        eval_steps = args.eval_tokens // (args.device_batch_size * args.max_seq_len * ddp_world_size)
        with autocast_ctx:
            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)
        print0(f"Step {step:05d} | Validation bpb: {val_bpb:.4f}")
        if val_bpb < min_val_bpb:
            min_val_bpb = val_bpb
        wandb_run.log(
            {
                "step": step,
                "total_training_flops": flops_so_far,
                "total_training_time": total_training_time,
                "val/bpb": val_bpb,
            }
        )
        model.train()

    # save checkpoint at the end of the run (only on master process)
    if master_process and last_step and not args.dry_run:
        output_dirname = args.output_tag or args.model_tag or f"s{size}"  # e.g. s12
        checkpoint_dir = os.path.join(base_dir, "chatsft_checkpoints", output_dirname)
        save_checkpoint(
            checkpoint_dir,
            step,
            orig_model.state_dict(),
            None,  # note: we don't save optimizer state
            {
                "step": step,
                "val_bpb": val_bpb,  # loss at last step
                "model_config": asdict(model.config),
                "user_config": user_config,  # inputs to the training script
            },
        )

    if last_step:
        break

    # -------------------------------------------------------------------------
    # single training step
    # evaluate the gradient
    synchronize()
    t0 = time.time()

    # Pre-sample all num_recur values for this step (global across all ranks)
    sampled_num_recurs = sample_num_recurs_for_step(
        recur_samples_per_step=args.recur_samples_per_step,
        mean_recur=model.config.train_recur_mean,
        sigma=0.5,
        min_recur=model.config.train_recur_min,
        max_recur=model.config.train_recur_max,
        ddp=ddp,
        master_process=master_process,
        device=device,
    )

    for _micro_step in range(grad_accum_steps):
        # Get num_recur for this micro-step
        num_recur = get_num_recur_for_microstep(
            sampled_num_recurs=sampled_num_recurs,
            micro_step=_micro_step,
            ddp_rank=ddp_rank,
            ddp_world_size=ddp_world_size,
            grad_accum_steps=grad_accum_steps,
            recur_samples_per_step=args.recur_samples_per_step,
        )

        with autocast_ctx:
            loss = model(x, y, num_recur=num_recur)
        train_loss = loss.detach()  # for logging
        loss = loss / grad_accum_steps  # each .backward() is a grad sum => normalize loss here
        loss.backward()
        x, y = next(train_loader)  # prefetch the next batch while the GPU is busy with forward/backward
        progress = max(progress, approx_progress)  # only increase progress monotonically

    # Compute model health statistics: gradients and parameters (after all backward passes complete)
    model_health_stats = compute_gradient_stats(orig_model, args.track_gradients)

    # step the optimizer
    lrm = get_lr_multiplier(progress)
    muon_momentum = get_muon_momentum(step)
    for group in optimizer.param_groups:
        group["lr"] = group["initial_lr"] * lrm
        if group.get("kind") == "muon":
            group["momentum"] = muon_momentum
    optimizer.step()
    model.zero_grad(set_to_none=True)

    # Extract effective learning rates from optimizer groups (for logging)
    # Groups order: [lm_head (unembed), embedding, norms, muon_shape1, muon_shape2, ...]
    effective_lr_unembed = optimizer.param_groups[0]["lr"]  # lm_head (AdamW)
    effective_lr_embed = optimizer.param_groups[1]["lr"]    # embedding (AdamW)
    effective_lr_muon = optimizer.param_groups[3]["lr"]     # first Muon group (all Muon groups have same lr)

    synchronize()
    t1 = time.time()
    dt = t1 - t0
    # -------------------------------------------------------------------------

    # State
    step += 1

    # logging
    train_loss_f = train_loss.item()  # raw unsmoothed loss from last microbatch
    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss_f  # EMA the training loss
    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta ** (step + 1))  # debias the EMA
    pct_done = 100 * progress
    tok_per_sec = int(args.total_batch_size / dt)
    flops_per_sec = num_flops_per_token * args.total_batch_size / dt
    promised_flops_per_sec_h100 = 989e12 * ddp_world_size  # bfloat16 H100 SXM and without 2:4 sparsity
    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100  # in %
    # For logging: report num_recur info
    if sampled_num_recurs is None:
        # Fixed: use model default
        logged_num_recur = int(model.config.train_recur_mean)
        logged_num_recur_str = f"{logged_num_recur}"
    else:
        # Sampled: report all values and mean
        logged_num_recur = sum(sampled_num_recurs) / len(sampled_num_recurs)  # mean for wandb
        logged_num_recur_str = f"{sampled_num_recurs} (mean={logged_num_recur:.1f})"
    if step > 10:
        total_training_time += dt  # only count the time after the first 10 steps
    print0(
        f"step {step:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} | lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | epoch: {current_epoch} | num_recur: {logged_num_recur_str} | total time: {total_training_time / 60:.2f}m"
    )
    if step % 10 == 0:
        log_data = {
                "step": step,
                "total_training_flops": flops_so_far,
                "total_training_time": total_training_time,
                "train/loss": debiased_smooth_loss,
                "train/loss_raw": train_loss_f,  # raw unsmoothed loss from last microbatch
                "train/lrm": lrm,
                "train/dt": dt,
                "train/tok_per_sec": tok_per_sec,
                "train/mfu": mfu,
                "train/epoch": current_epoch,
                "train/num_recur": logged_num_recur,
                "lr/muon": effective_lr_muon,       # effective learning rate for Muon (matrix params)
                "lr/embed": effective_lr_embed,     # effective learning rate for embeddings
                "lr/unembed": effective_lr_unembed, # effective learning rate for unembedding (lm_head)
                **{f"model_health/{k}": v for k, v in model_health_stats.items()},  # Add model health stats
        }
        if orig_model.config.use_exit_gate and args.exit_log_stats:
            stats_batch = min(2, x.size(0))
            stats_x = x[:stats_batch]
            stats_y = y[:stats_batch]
            with torch.no_grad(), autocast_ctx:
                gate_stats = orig_model.compute_exit_stats(stats_x, targets=stats_y, num_recur=int(logged_num_recur))
            log_data.update(
                {
                    "gate/entropy": gate_stats["entropy"].item(),
                    "gate/expected_t": gate_stats["expected_t"].item(),
                    "gate/p_last": gate_stats["p_last"].item(),
                }
            )
        wandb_run.log(log_data)

# print a few more stats
print0(f"Peak memory usage: {get_max_memory() / 1024 / 1024:.2f}MiB")
print0(f"Total training time: {total_training_time / 60:.2f}m")
print0(f"Minimum validation bpb: {min_val_bpb:.4f}")

# Log to report
if not args.dry_run:
    from nanochat.report import get_report

    get_report().log(
        section="SFT",
        data=[
            user_config,  # CLI args
            {  # stats about the training setup
                "Number of iterations": step,
                "DDP world size": ddp_world_size,
            },
            {  # stats about training outcomes
                "Minimum validation bpb": min_val_bpb,
            },
        ],
    )

# cleanup
wandb_run.finish()  # wandb run finish
compute_cleanup()
